{"cells":[{"metadata":{"_uuid":"91f6e9e76f634c5cea62b652ca20232a8beb71cb","_cell_guid":"9d6a6163-6545-4d07-9caa-9e1c972db380"},"cell_type":"markdown","source":"# Table of Contents\n___\n\n-[Introduction](#Introduction)\n\n-[Data Exploration](#Data-Exploration)\n\n-[Understanding russian using googletrans](#Understanding-russian-using-googletrans)\n\n-[Time series analysis](#Time-series-analysis)\n\n-[Conclusions](#Conclusions)"},{"metadata":{"_uuid":"5716007ae83a0fb0f7e65096cdab517bbf203842","_cell_guid":"7ce411e7-c484-4e5e-8c5b-61bf259558da"},"cell_type":"markdown","source":"## Introduction\n___\n\nIn this kernel, is to make an EDA (exploration data analysis), focusing on how can we get information after using `googletrans` library to translate all the russian words in this dataset. The translation is done two times: one with a smaller dataframe and another with a larger dataframe (the only difference is the time of execution). For the larger, there are a few additions to try to optimize reduce the translation time. We'll also see a time series decomposition using the `statsmodel` library.\n\nFirst we import the libraries and get each dataset."},{"metadata":{"collapsed":true,"_uuid":"facbbd93160e5b5c6861c9908dcb84921c5e48a8","_cell_guid":"06b5f7f4-4df9-4e6e-bb6b-3894296f470d","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport scipy.stats as stats\nimport matplotlib.pyplot as plt\n%matplotlib inline \n#shows plot from matplotlib and seaborn in the Jupyter notebook","execution_count":3,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"bf2564bb1a14f77a7be25ab7152a7766850a2567","_cell_guid":"cc863de3-40ea-4ee5-a1ab-ee4877ee64c1","trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/sales_train.csv\")\nitems = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/items.csv\")\nitem_categories = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/item_categories.csv\")\nshops = pd.read_csv(\"../input/competitive-data-science-predict-future-sales/shops.csv\")\n","execution_count":4,"outputs":[]},{"metadata":{"_uuid":"b68cc2546332a2434fa7e1cbf5bcfac620095c77","_cell_guid":"8f7da13d-213d-41b5-a1a4-72b26b6f744a","trusted":true},"cell_type":"code","source":"df.head()\n#shops.head()\n#items.head()\n#item_categories.head()","execution_count":5,"outputs":[]},{"metadata":{"_uuid":"13f96ea9a98f4cb57e437ac16ba010376a51a7be","_cell_guid":"390b6276-2f6f-44f3-b5d8-f454b2dda1f8"},"cell_type":"markdown","source":"The main dataset doesn't tell us much, but we can marge this dataset with the other two, so we get more information."},{"metadata":{"_uuid":"0d044a96b6828a02a93ffa31529d4d91357ffa56","_cell_guid":"fa6fed53-9e56-4b84-bdaa-41071bd5c072","trusted":true},"cell_type":"code","source":"df = pd.merge(df, items, on=\"item_id\")\n\ndf = pd.merge(df, item_categories, on=\"item_category_id\")\n\ndf.head()","execution_count":6,"outputs":[]},{"metadata":{"_uuid":"fa957ea95386cb719c9281cb61c5d9f23db16428","_cell_guid":"08c5afa3-242b-4dd9-b37d-db5286e9539f"},"cell_type":"markdown","source":"Well, now it's much better! Some information is written in russian, so we will have to deal with it later. For now, let's see how the sales, that we should get from item_price, change as the time passes.\n\n## Data Exploration\n___\n\nThere is a catch in this dataset. The days are written in the form day-month-year, rather than the usual month-day-year very common in english-written datasets. (Thanks @anatoly for calling my attention to that!) We now group by date and sum over the `item_price` and extract day, month and year from the `date` column."},{"metadata":{"_uuid":"9c97dfcbe7a367405e6c488a6643ead598ddb12e","_cell_guid":"8772bb84-0b93-4a6c-92f7-a3e85947bacd","trusted":true},"cell_type":"code","source":"revenueByDate = pd.DataFrame(df.groupby('date', as_index=False)['item_price'].sum())\nrevenueByDate[\"day\"] = revenueByDate.date.str.extract(\"([0-9][0-9]).\", expand = False)\nrevenueByDate[\"month\"] = revenueByDate.date.str.extract(\".([0-9][0-9]).\", expand = False)\nrevenueByDate[\"year\"] = revenueByDate.date.str.extract(\".([0-9][0-9][0-9][0-9])\", expand =False)\nrevenueByDate.head()","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"5c318df73b3c2f524c4b3b37b1254f67e4d8db99","_cell_guid":"90174b7b-6a4e-4c50-8c64-9ae6a3c56749","trusted":true},"cell_type":"code","source":"revenueByDate[\"date\"] = pd.to_datetime(revenueByDate[[\"year\", \"month\", \"day\"]])\n\nplt.plot( \"date\", \"item_price\", data = revenueByDate.sort_values(by=\"date\"))\nplt.show()","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"9cff36e7cbfe3cb4d0cc1fd7633759b48cb9411a","_cell_guid":"575ccb65-e0b5-4334-a1bd-742b45e28d17"},"cell_type":"markdown","source":"Not as enlightening as I expected! The data is too noisy in this plot, as it accounts for the variation of each day's sales. It may be good if we make this plot monthly rather than daily, so we'll be able to see a less noisy graph and maybe get some intuition about our series.\n\nBut first, we will have to create a new date column so we will have only months and years."},{"metadata":{"_uuid":"4116f3ae707033e2a0cc7fd425ae1f508c0746a6","_cell_guid":"7e94e793-b61f-4601-8295-5a3954cffbfb","trusted":true},"cell_type":"code","source":"revenueByDate[\"day\"] = 1 #rewrite this column using ones so I can use the DatetimeIndex and aggregate sales by month\nrevenueByDate['monthlyDate'] = pd.to_datetime(revenueByDate[[\"year\", \"month\", \"day\"]])\n\nrevenueByDate.head()","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"e00edd849bba47c75dbf79be07c6bc9294c5f2f1","_cell_guid":"d3b9de44-d232-4152-9736-19bb4f09b911","trusted":true},"cell_type":"code","source":"revenueByMonth = pd.DataFrame(revenueByDate.groupby(\"monthlyDate\", as_index=False)[\"item_price\"].sum())\nrevenueByMonth.head()","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"f71609dfb00bc929d01e93cefae10ce8f7e9814f","_cell_guid":"17867016-00dc-443b-8856-0c07c614e9a9"},"cell_type":"markdown","source":"Great! Now we have the total sales for each month. What does this series looks like?"},{"metadata":{"_uuid":"9da7efaa601eb0f0bd01fc28074288d3685f438f","_cell_guid":"49bbc245-dba9-435b-a047-6f4f738e8643","trusted":true},"cell_type":"code","source":"plt.plot(\"monthlyDate\", \"item_price\", data = revenueByMonth.sort_values(by=\"monthlyDate\"))\nplt.xticks(rotation = 90);","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"000a284ad8c02d5c06fe395212f91d5537dad51d","_cell_guid":"995671d5-b168-41e5-83d3-cf50f7ff8abb"},"cell_type":"markdown","source":"Wow! Now it is much more clear how the sales behaved during all the period of records. We can see two peaks in sales, one in Dec 2013 and another in Dec 2014. Those peaks can be confirmed using `nlargest` method as indicated below."},{"metadata":{"_uuid":"57bc3d98ca4f93522992c4fb7b2e0538e30ed163","_cell_guid":"ef52c725-760c-4516-bc4c-9f0de338746e","trusted":true},"cell_type":"code","source":"revenueByMonth[revenueByMonth[\"item_price\"] == revenueByMonth[\"item_price\"].max()]\n\n#gets the 2 highest item prices and their respective row index.\nrevenueByMonth[\"item_price\"].nlargest(2)\n\nrevenueByMonth.iloc[[11,23],:]","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"493dd2124f529f6861d4cbfdcec155360107ef98","_cell_guid":"5daffd6a-6252-43bd-a35f-35674c2d7d1a"},"cell_type":"markdown","source":"We've just seen how sales behaved through the years, but how do monthly revenue looks like? "},{"metadata":{"_uuid":"26ae5740990a95d0a8966f986943a4a270cc647f","_cell_guid":"a47d85cd-a019-4cc0-aa36-5e8a6d0be84b","trusted":true},"cell_type":"code","source":"monthlyRev = pd.DataFrame(revenueByDate.groupby([\"month\", \"year\"], as_index=False)[\"item_price\"].sum())\nmonthlyRev.head()\n\n\ng = sns.FacetGrid(data = monthlyRev.sort_values(by=\"month\"), hue = \"year\", size = 5, legend_out=True)\ng = g.map(plt.plot, \"month\", \"item_price\")\ng.add_legend()\ng;","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"b4448ec02666196a76e387af73791fc5bdc3f539","_cell_guid":"882d496b-541a-4ff7-9032-f3c82848eb7a"},"cell_type":"markdown","source":"2014's sales were higher than 2013's in all months, but as we've seen before, 2015 wasn't a good year for this store. Sales went lower than the 2 previous years just after January.\n\nBefore proceeding in our analysis, we should change the data type of some columns."},{"metadata":{"_uuid":"c31faec6feb402703451618c5856eee11def8f68","_cell_guid":"4444ae7f-a85c-498e-8c59-e807712cf758","trusted":true},"cell_type":"code","source":"df[\"item_id\"] = df[\"item_id\"].astype(\"category\")\ndf[\"item_name\"] = df[\"item_id\"].astype(\"category\")\ndf[\"item_category_id\"] = df[\"item_category_id\"].astype(\"category\")\ndf[\"item_category_name\"] = df[\"item_category_name\"].astype(\"str\")\n#apparently, if item_category_name is set as category, we cannot use googletrans in the next section\n\ndf.info()","execution_count":14,"outputs":[]},{"metadata":{"_uuid":"88a4e409090510e6e7ed9e1f239e760d430a61b7","_cell_guid":"6dd4f476-565d-41d8-8e6c-f7894c21db3c"},"cell_type":"markdown","source":"We can create a new dataframe that has the total revenue over the years for each item category. I'll limit our dataframe to the 15 most rentable categories for analysis for simplicity."},{"metadata":{"_uuid":"d9f6f0701408cf2a62f147ef2e1ce92b8eebe2a6","_cell_guid":"ff3d5168-0755-4289-b9ca-cf03d83a671f","trusted":true},"cell_type":"code","source":"sales_by_category = df.groupby(\"item_category_name\", \n                             as_index = False)[\"item_price\"].sum().sort_values(by = \"item_price\")\n\ntop_sales = sales_by_category.nlargest(n=15, columns=[\"item_price\"])\n\ntop_sales.set_index(np.arange(0,len(top_sales),1))","execution_count":15,"outputs":[]},{"metadata":{"_uuid":"686c4e48d3cd95a5c8dcc7d32c804003af49ce71","_cell_guid":"ac15820e-17d3-44d9-95d2-344047605194"},"cell_type":"markdown","source":"Oh look! I recognize some words! I know what an Xbox, PS, DVD, PC, Blu-ray and CD are! But... that's all. How can we get information if we don't understand what is being written? Fear not, young grasshopper! I'll guide you with the help of Google (yeah.. always them)."},{"metadata":{"_uuid":"b0ef827948b9d33f82c0d7ec8344741c73ee6c69","_cell_guid":"5431d09d-2f92-451e-8674-11d2360b6272"},"cell_type":"markdown","source":"## Understanding russian using googletrans\n___\n\nWe finally arrived in this section! I cannot understand russian, although I wish I could, and I think many people here are in the same situation. In order to know what we are dealing with, we should translate those category names. Here I'll use googletrans library in order to provide a fair translation of the content. As we all know, google translations still have a way to go in some languages (translation in Portuguese, my mother language, are terrible sometimes), but hopefully the translation will be good enough to at least provide a basic understanding of the categories.\n\nHere I'll loop through every row in top_sales dataframe and translate the categories while rewriting them. At the end, I'll use a barplot to visualize top_sales dataframe."},{"metadata":{"_uuid":"3c76adb46bf94c4c9c7f68b6c26c811ef8434169","_cell_guid":"5e30a210-12de-403b-a316-85cc54468786","trusted":true},"cell_type":"code","source":"#The code below is used to translate each row of \"item_category_name\"\n#However, this library needs to use the internet (to access Google)\n#in order to make its translation. Kaggle doesn't let kernels to access\n#the web, so in order to overcome this issue, I've uploaded top_sales\n#dataframe after the translation, the same you should get after running this \n#piece of commented code in your local machine\n'''\nfrom googletrans import Translator\n\ntranslator = Translator()\n\ni = 0\nfor row in top_sales[\"item_category_name\"]:\n    english_word = translator.translate(row)\n    top_sales.iloc[i,0] = english_word.text\n    i+=1\n'''\n\ntop_sales = pd.read_csv(\"../input/additional-data-for-competition/topsales.csv\")\n\nsns.barplot(y = \"item_category_name\", x = \"item_price\",\n             data = top_sales)\nplt.title(\"Sales for each one of the top 15 categories-products\")\nplt.xlabel(\"Sales\")\nplt.ylabel(\"Category-Product\")","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"2f1df794c6ce9d2ec6a034ed36171d3290b717f9","_cell_guid":"556329de-186c-4a92-899a-bf374ea15544"},"cell_type":"markdown","source":"It is clear that most sales are related to eletronics, especially videogames and consoles. Perhaps the 2015 drop on the previous plot was because of a sharp cut in the purchases of game-related products. We can check it in a moment.\n\nInstead of using the \"category - product\" format, I'll use only the category, so we can condense information a little bit."},{"metadata":{"_uuid":"e1bcef05204da479f3ade0a03231dd5d0c8884d9","_cell_guid":"799e5e41-d3bf-46f1-950d-e6d630d237e3","trusted":true},"cell_type":"code","source":"top_sales[\"item_category\"] = top_sales.item_category_name.str.extract('([A-Za-z\\ ]+)', expand=False) \n\ntop_sales.head()","execution_count":17,"outputs":[]},{"metadata":{"_uuid":"072a1eb5d6bd13ea2637776dad1637059e5b70b3","_cell_guid":"7218dfb4-0300-4477-8d13-9859ff3e98a0"},"cell_type":"markdown","source":"It seems we got what we wanted. Now let's make the same plot as before:"},{"metadata":{"_uuid":"0bd357e2b50d87f4f56b8ccd09bb38ace2f24460","_cell_guid":"b2612676-bf88-48a4-8f90-6baca4871ff2","trusted":true},"cell_type":"code","source":"sns.barplot(y = \"item_category\", x = \"item_price\",\n             data = top_sales)\nplt.title(\"Sales for each one of the top 8 categories\")\nplt.xlabel(\"Sales\")\nplt.ylabel(\"Category-Product\")","execution_count":18,"outputs":[]},{"metadata":{"collapsed":true,"_uuid":"a09ac5f4bc2f9a113c345abaabde0e31a20a5687","_cell_guid":"67a36549-e3cb-41f5-a340-7ce965599b51"},"cell_type":"markdown","source":"This plot says what we already figured out from the previous barplot. Most sales are game-related. \n\nNow we could make a time series plot over each category and see the increase or decrease in sales over the years. First, though, we have to make a similar translation for all the dataset.\n\nThis time, I'm using the counts of items that were sold, rather than the total price, so I won't consider possible changes in prices through, but only the amount of itens sold."},{"metadata":{"_uuid":"7f9e60603282bc2f0bc1d950fd7c47827ed7209b","_cell_guid":"cf7e6751-5b7b-47c3-89c8-c16505170891","trusted":true},"cell_type":"code","source":"dailyItensByCat = pd.DataFrame(df.groupby([\"item_category_name\", \"date\"], as_index=False)[\"item_cnt_day\"].sum())\n\ndailyItensByCat[\"month\"] = dailyItensByCat.date.str.extract(\".([0-9][0-9]).\", expand = False)\ndailyItensByCat[\"year\"] = dailyItensByCat.date.str.extract(\".([0-9][0-9][0-9][0-9])\", expand =False)\ndailyItensByCat[\"day\"] = 1 #create this column so I can use the DatetimeIndex\ndailyItensByCat['monthlyDate'] = pd.to_datetime(dailyItensByCat[[\"year\", \"month\", \"day\"]])\n\nmonthlyItensByCat = pd.DataFrame(dailyItensByCat.groupby([\"monthlyDate\", \"item_category_name\"],\n                                                         as_index = False)[\"item_cnt_day\"].sum())\nmonthlyItensByCat.head()","execution_count":19,"outputs":[]},{"metadata":{"_uuid":"5b66ae2a85ac971bb1fb2cf7fd40d51df0e8e7da","_cell_guid":"3e16685d-1f31-4599-a192-0b559d2757c5","trusted":true},"cell_type":"code","source":"monthlyItensByCat = pd.DataFrame(dailyItensByCat.groupby([\"monthlyDate\", \"item_category_name\"],\n                                                         as_index = False)[\"item_cnt_day\"].sum())\n\nmonthlyItensByCat.head()","execution_count":20,"outputs":[]},{"metadata":{"_uuid":"800a256bd920a5b8912f5c69485405ab7da8318d","_cell_guid":"ae2705b9-7934-4374-85ca-e3ef49a92174","trusted":true},"cell_type":"code","source":"monthlyItensByCat[\"item_category\"] = (\n    monthlyItensByCat.item_category_name.str.extract(r'((?i)[А-Яa-я\\ ]+)', expand=False))\n\nmonthlyItensByCat.head()\n\n#As a record: I've spent hours looking for a way to extract \n#cyrillic alphabet, but there is no simple answer on the web.\n#In the end, turned out the solution was simple indeed and achieved by \n#trial and error. I hope this will help someone when they need a way\n#to extract cyrillic alphabet words so they won't spend such a long time\n#looking for an answer :P \n\n","execution_count":21,"outputs":[]},{"metadata":{"_uuid":"17317cf40f6f066ef601c9fe0ef082413063a6ee","_cell_guid":"dbb1a645-c2cd-48cd-a10c-67e51437e58d"},"cell_type":"markdown","source":"Again, we keep grouping. Now, we group by the new general category `item_category` and month."},{"metadata":{"_uuid":"5c4c998d89c6847671bf0e899ac9ca6664008a27","_cell_guid":"b811d4d9-8975-406d-8138-c35a2089af1a","trusted":true},"cell_type":"code","source":"countByCatByMonth = monthlyItensByCat.groupby([\"item_category\", \"monthlyDate\"], as_index=False)[\"item_cnt_day\"].sum()\ncountByCatByMonth['item_category_trans'] = None\ncountByCatByMonth.head()","execution_count":22,"outputs":[]},{"metadata":{"_uuid":"29e16d1043b29a0ea6ab2bf8f3a4dcf07dc7c6e3","_cell_guid":"78271614-0e20-4866-9043-88efb6bc374c"},"cell_type":"markdown","source":"Finally we can translate the new dataset. Here I've made a slight change in the code from before. Since googletrans uses the internet to identify the language and translate it (you can set the language in your code, by the way), checking everytime for the same word sounds very time consuming. So I've created a dictionary to store the words that were already translated and only get them rather them querying google translator again. "},{"metadata":{"_uuid":"150d14729633d2c4e20817baab9fed505a37e2ee","_cell_guid":"6979295f-574a-45bc-8080-c8adb82ba472","trusted":true},"cell_type":"code","source":"#Here again, we need internet access to perform the translations\n#and Kaggle doesn't let us. So I uploaded the exact dataframe that\n#should be generated after running the code below\n\n'''\nfrom googletrans import Translator\n\ncat_translator = Translator()\n\nobs = 0\nalready_translated = {'word': [], 'translation':[]}\n#creates a new column in countByCatByMonth\ncountByCatByMonth['item_category_trans'] = None\n\n#for each row...\nfor cat_name in countByCatByMonth[\"item_category\"]:\n    \n    #check if it has already been translated\n    if cat_name in already_translated['word']:\n        #if it is, get the index of the original word on the dictionary...\n        word_index = already_translated[\"word\"].index(cat_name)\n        #and use it to get the translated word\n        countByCatByMonth.iloc[obs,3] = already_translated[\"translation\"][word_index]\n        #if the word was not translated yet...\n    else:\n        try:\n            #translate it\n            english_word = cat_translator.translate(cat_name)\n            #append in dictionary for later use\n            already_translated['word'].append(cat_name)\n            already_translated['translation'].append(english_word.text)\n            #write the translated word into dataframe\n            countByCatByMonth.iloc[obs,3] = english_word.text\n    \n        except:\n            print (\"Error in row \"+ cat_name)\n    obs+=1\n'''\ncountByCatByMonth = pd.read_csv(\"../input/additional-data-for-competition/countbycatbymonth.csv\")\ncountByCatByMonth.head()","execution_count":23,"outputs":[]},{"metadata":{"_uuid":"fe24ed5f858ae6a75f94eada6da24cc49a9af973","_cell_guid":"452cbb0f-62d2-48d6-91ae-31893d68743d","trusted":true},"cell_type":"code","source":"g = sns.FacetGrid(data = countByCatByMonth.sort_values(by=\"monthlyDate\"), hue = \"item_category_trans\", legend_out=True, size = 8)\ng = (g.map(plt.plot, \"monthlyDate\", \"item_cnt_day\").set(xticks=[0, 5, 11, 17, 23, 29, 35],\n                                                        xticklabels=['2013-01-01', '2013-06-01', '2013-12-01', \n                                                                     '2014-06-01','2014-12-01', '2015-06-01','2015-12-01']))\ng.add_legend()\n\n","execution_count":24,"outputs":[]},{"metadata":{"_uuid":"59d6e12edf1993fc8d51da9241172f93b5375a4b","_cell_guid":"c989a148-8255-4e54-ad06-c07d75624b1d"},"cell_type":"markdown","source":"What a pretty good looking plot! From it we can see that items from those most profitable categories had a cut in sells. People had stopped buying such products since the beginning of 2014, although some itens had a sharper decrease than others."},{"metadata":{"_uuid":"af505bed9264ceb53034528888e36c607fcf4d41","_cell_guid":"99105356-7d73-457a-b1e3-593d310d33df"},"cell_type":"markdown","source":"## Time series analysis\n___\n\nFirst I'll analyse the item_price series. To make our analysis, it is best to set the dates as index, so on the series decomposition they will show up in x-axis."},{"metadata":{"_uuid":"b5292b6bd0d37dd0372dcdfe62bc8dc90640f16b","_cell_guid":"e7298474-913d-4fed-ae63-608655db8ca3","trusted":true},"cell_type":"code","source":"ts = revenueByMonth.set_index(\"monthlyDate\")\nts.head()","execution_count":25,"outputs":[]},{"metadata":{"_uuid":"bfbe400514fa7b4cb33c4d3743738c766137666f","_cell_guid":"e10e4d62-1e65-4e03-b95c-25226d551144"},"cell_type":"markdown","source":"First let's take a look at the decomposition of the item_price time series. We will split our time series plot in three components: trend, seasonality and residual error (the part of the time series that cannot be explained by the former two). After that, we plot each component and analyse what we got."},{"metadata":{"_uuid":"21471b7ddcf4a42079c169296f4ece1edf57985e","_cell_guid":"a5bfefd8-a019-4003-b80f-22155f35ae8c","trusted":true},"cell_type":"code","source":"from statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(ts)\n\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nf, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, sharex='col', sharey = 'row')\nax1.plot(ts)\nax1.set_title(\"original\")\nax2.plot(trend)\nax2.set_title(\"trend\")\nax3.plot(seasonal)\nax3.set_title(\"seasonal\")\n\nax4.plot(residual)\nax4.set_title(\"residual\")\nf.set_figheight(7)\nf.set_figwidth(10)\n\n#let me rotate the labels of x-axis. \n#Otherwise they get mixed and very hard to read.\nfor ax in f.axes:\n    plt.sca(ax)\n    plt.xticks(rotation = 45)","execution_count":26,"outputs":[]},{"metadata":{"_uuid":"a2512a154d69bb4adce02504b6eae893e7e4893d","_cell_guid":"51a1afa1-0905-463d-8590-aca4f94bb67b"},"cell_type":"markdown","source":"In this decomposition, we can describe the behaviour of the most important features of our timeseries. We can clearly see that sales increase every december. When we decompose a time series, the algorithm will always try to find a seasonality. So in order to confirm if there is or not a real seasonality, we should see both residual and seasonal plots and compare the scales. Sometimes, the residuals magnitude is way above seasonal magnitude, which means that if there is a seasonal effect, it is negligible. Here, however, seasonality does indeed exists, as the order of magnitude is the same (1e7) and the max value of the seasonality effect in sales is about 4 times the highest residual. \n\nTrend goes up until July 2014 and then starts to drop and keep this way as by the end of the records. \n\nResidual plot show that there are two key points in time where neither trend nor seasonal effects can explain the sales - Dec 2013 and Dec 2014. In the former, sales were lower than expected by the trend at the time and the seasonal effect, while on the latter, sales were higher than expected.\n\n## Conclusions\n___\n\nIn this notebook, we've seen how to deal with data in another language using `googletrans` package. Some exploratory analysis was done and it was seen that some categories were sold more frequently than others and that by mid-2014 sales had started dropping.  We've also made a basic decomposition of the `item_price` time series and confirmed that there is indeed a seasonality (sales in December are increased by a lot). \n\nI'm ending the notebook here but as soon as I learn more about time series, I'll come back and finish this notebook with a forecast. I'm planning to use an ARIMA model to make the it. If you liked, please upvote! Any suggestions to improve the notebook or doubts please tell me on the comments!"},{"metadata":{"collapsed":true,"_uuid":"63b1ff86b13f16536b3e9352b5ababc0e8b7e817","_cell_guid":"68e1a614-c049-4212-bd15-e667bea6f24e","trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}